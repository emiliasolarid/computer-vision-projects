{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zemWtDr8gVOf"
      },
      "source": [
        "\n",
        "# Building and Optimizing a CNN\n",
        "\n",
        "In this homework, you will design, implement, and optimize a **Convolutional Neural Network (CNN)** using PyTorch to classify images from the CIFAR-10 dataset. This will involve advanced preprocessing techniques, sophisticated model architectures, hyperparameter tuning, and comprehensive evaluation.\n",
        "\n",
        "## Project Overview\n",
        "- **Task**: Image classification on CIFAR-10 dataset\n",
        "- **Architecture**: CNN with Inception bottlenecks and advanced optimizations\n",
        "- **Dataset**: CIFAR-10 (60,000 32x32 color images in 10 classes)\n",
        "- **Goal**: Achieve high classification accuracy with optimized training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJXznwbCEZB"
      },
      "source": [
        "## Import Libraries and Configuration\n",
        "\n",
        "\n",
        "**Requirements**:\n",
        "- Import PyTorch, torchvision, and related libraries\n",
        "- Import matplotlib, numpy, and other utilities\n",
        "- Set random seeds for reproducibility\n",
        "- Configure hyperparameters with reasonable values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMI1B93bCEZB"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision torchmetrics matplotlib numpy scikit-learn roboflow torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOQOOq3YCEZC"
      },
      "outputs": [],
      "source": [
        "#  Import all necessary libraries:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report\n",
        "import copy\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Torch sets for reproducibility:\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "\n",
        "# Check device availability and print\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define configuration parameters:\n",
        "BATCH_SIZE = 128  # Batch size for training\n",
        "LEARNING_RATE = 0.001  # Initial learning rate\n",
        "NUM_EPOCHS = 50  # Number of training epochs\n",
        "NUM_CLASSES = 10  # CIFAR-10 has 10 classes\n",
        "INPUT_SIZE = 32  # CIFAR-10 image size is 32x32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvBM5CoKCEZC"
      },
      "source": [
        "## Load and Preprocess the Data\n",
        "\n",
        "**Task**: Load CIFAR-10 dataset and implement advanced preprocessing techniques.\n",
        "\n",
        "**Requirements**:\n",
        "- Load CIFAR-10 training and test sets\n",
        "- Apply data normalization using dataset statistics\n",
        "- Implement comprehensive data augmentation for training\n",
        "- Create data loaders with appropriate settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhZTgTLHCEZD"
      },
      "outputs": [],
      "source": [
        "# Define data transforms for training (with augmentation):\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flip with 50% probability -> model learn orientation invariance\n",
        "    transforms.RandomRotation(degrees=10),  # Random rotation within 10 degrees -> model learn rotation invariance\n",
        "    transforms.RandomCrop(32, padding=4),  # Random crop with padding -> model learn translation invariance\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Randomly change brightness, contrast, saturation, and hue -> model learn color invariance\n",
        "    transforms.ToTensor(),  # Convert PIL images to tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize images with CIFAR-10 mean and std -> model learn normalized data\n",
        "])\n",
        "\n",
        "# Define data transforms for testing (no augmentation):\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert PIL images to tensors\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 datasets:\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "# Create data loaders:\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# Define CIFAR-10 class names\n",
        "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "# Print dataset information (sizes, classes, etc.)\n",
        "print(f\"Number of training samples: {len(trainset)} , Number of test samples: {len(testset)}, Size of each image: {trainset[0][0].shape}, Number of classes: {len(CIFAR10_CLASSES)}\")\n",
        "\n",
        "# Visualize some sample images with their labels\n",
        "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
        "axes = axes.flatten()\n",
        "\n",
        "examples = iter(trainloader)\n",
        "images, labels = next(examples)\n",
        "\n",
        "# Plot images with their labels\n",
        "for i in range(15):\n",
        "    # Unnormalize the images for visualization\n",
        "    img = images[i].permute(1, 2, 0).cpu().numpy()\n",
        "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    axes[i].imshow(img)\n",
        "    axes[i].set_title(CIFAR10_CLASSES[labels[i]])\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h0SA1ZBCEZD"
      },
      "source": [
        "## 3️⃣ Design Complex CNN Architecture with Inception Bottlenecks\n",
        "\n",
        "**Task**: Implement a sophisticated CNN architecture incorporating Inception-style bottleneck blocks.\n",
        "\n",
        "**Requirements**:\n",
        "- Create an Inception bottleneck module with multiple parallel paths\n",
        "- Design the main CNN with multiple Inception blocks\n",
        "- Use appropriate pooling, batch normalization, and dropout\n",
        "- Implement skip connections where beneficial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viAGV6lhCEZD"
      },
      "outputs": [],
      "source": [
        "# Create InceptionBottleneck class inheriting from nn.Module\n",
        "class InceptionBottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    Inception-style bottleneck module with four parallel paths:\n",
        "    1. 1x1 convolution\n",
        "    2. 1x1 reduction -> 3x3 convolution\n",
        "    3. 1x1 reduction -> 5x5 convolution\n",
        "    4. 3x3 max pooling -> 1x1 projection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_1x1, reduce_3x3, out_3x3, reduce_5x5, out_5x5, out_pool):\n",
        "        super(InceptionBottleneck, self).__init__()\n",
        "\n",
        "        # Path 1: 1x1 convolution\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_1x1, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_1x1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Path 2: 1x1 reduction + 3x3 convolution\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, reduce_3x3, kernel_size=1),\n",
        "            nn.BatchNorm2d(reduce_3x3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(reduce_3x3, out_3x3, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_3x3),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Path 3: 1x1 reduction + 5x5 convolution\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, reduce_5x5, kernel_size=1),\n",
        "            nn.BatchNorm2d(reduce_5x5),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(reduce_5x5, out_5x5, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(out_5x5),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        # Path 4: 3x3 max pooling + 1x1 projection\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, out_pool, kernel_size=1),\n",
        "            nn.BatchNorm2d(out_pool),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    # Forward method for InceptionBottleneck:\n",
        "    def forward(self, x):\n",
        "        # Pass input through all four paths\n",
        "        branch1x1_out = self.branch1x1(x)\n",
        "        branch3x3_out = self.branch3x3(x)\n",
        "        branch5x5_out = self.branch5x5(x)\n",
        "        branch_pool_out = self.branch_pool(x)\n",
        "\n",
        "        # Concatenate outputs along channel dimension\n",
        "        outputs = torch.cat([branch1x1_out, branch3x3_out, branch5x5_out, branch_pool_out], dim=1)\n",
        "        return outputs\n",
        "\n",
        "class InceptionCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN architecture with Inception bottleneck blocks designed for CIFAR-10 classification (32x32 RGB images)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(InceptionCNN, self).__init__()\n",
        "\n",
        "        # Initial convolutional layers\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 32x32 -> 16x16\n",
        "            nn.Dropout2d(0.1)\n",
        "        )\n",
        "\n",
        "        # First set of Inception blocks\n",
        "        self.inception1 = InceptionBottleneck(\n",
        "            in_channels=64,\n",
        "            out_1x1=16,\n",
        "            reduce_3x3=32, out_3x3=64,\n",
        "            reduce_5x5=16, out_5x5=32,\n",
        "            out_pool=32\n",
        "        )  # Output channels: 16 + 64 + 32 + 32 = 144\n",
        "\n",
        "        self.inception2 = InceptionBottleneck(\n",
        "            in_channels=144,\n",
        "            out_1x1=32,\n",
        "            reduce_3x3=64, out_3x3=128,\n",
        "            reduce_5x5=32, out_5x5=64,\n",
        "            out_pool=64\n",
        "        )  # Output channels: 32 + 128 + 64 + 64 = 288\n",
        "\n",
        "        self.pool1 = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16x16 -> 8x8\n",
        "            nn.Dropout2d(0.2)\n",
        "        )\n",
        "\n",
        "        # Second set of Inception blocks\n",
        "        self.inception3 = InceptionBottleneck(\n",
        "            in_channels=288,\n",
        "            out_1x1=64,\n",
        "            reduce_3x3=128, out_3x3=256,\n",
        "            reduce_5x5=64, out_5x5=128,\n",
        "            out_pool=128\n",
        "        )  # Output channels: 64 + 256 + 128 + 128 = 576\n",
        "\n",
        "        self.inception4 = InceptionBottleneck(\n",
        "            in_channels=576,\n",
        "            out_1x1=128,\n",
        "            reduce_3x3=256, out_3x3=512,\n",
        "            reduce_5x5=128, out_5x5=256,\n",
        "            out_pool=256\n",
        "        )  # Output channels: 128 + 512 + 256 + 256 = 1152\n",
        "\n",
        "        # Global average pooling instead of large fully connected layers\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(1152, num_classes)\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        \"\"\"Initialize network weights using Xavier/He initialization\"\"\"\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initial convolution layers\n",
        "        x = self.initial_conv(x)  # (batch, 64, 16, 16)\n",
        "\n",
        "        # First set of Inception blocks\n",
        "        x = self.inception1(x)    # (batch, 144, 16, 16)\n",
        "        x = self.inception2(x)    # (batch, 288, 16, 16)\n",
        "        x = self.pool1(x)         # (batch, 288, 8, 8)\n",
        "\n",
        "        # Second set of Inception blocks\n",
        "        x = self.inception3(x)    # (batch, 576, 8, 8)\n",
        "        x = self.inception4(x)    # (batch, 1152, 8, 8)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = self.global_avg_pool(x)  # (batch, 1152, 1, 1)\n",
        "\n",
        "        # Flatten for classifier\n",
        "        x = x.view(x.size(0), -1)   # (batch, 1152)\n",
        "\n",
        "        # Classification\n",
        "        x = self.classifier(x)      # (batch, num_classes)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def count_parameters(self):\n",
        "        \"\"\"Count total number of trainable parameters\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "# Initialize the model\n",
        "model = InceptionCNN(num_classes=10).to(device)\n",
        "print(f\"Model initialized with {model.count_parameters():,} parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wspd9xM3CEZE"
      },
      "source": [
        "##  Implement and Compare Different Optimizers\n",
        "\n",
        "**Task**: Set up multiple optimizers and compare their performance.\n",
        "\n",
        "**Requirements**:\n",
        "- Implement SGD, Adam, and AdamW optimizers\n",
        "- Use appropriate hyperparameters for each\n",
        "- Create a function to easily switch between optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R_RkTNeCEZE"
      },
      "outputs": [],
      "source": [
        "def get_optimizer(model, optimizer_name, learning_rate):\n",
        "    \"\"\"\n",
        "    Create optimizer based on name with appropriate hyperparameters\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model\n",
        "        optimizer_name: String name of optimizer ('sgd', 'adam', 'adamw')\n",
        "        learning_rate: Initial learning rate\n",
        "\n",
        "    Returns:\n",
        "        Configured optimizer\n",
        "    \"\"\"\n",
        "    if optimizer_name.lower() == 'sgd':\n",
        "        return optim.SGD(model.parameters(),\n",
        "                        lr=learning_rate,\n",
        "                        momentum=0.9,\n",
        "                        weight_decay=1e-4)\n",
        "\n",
        "    elif optimizer_name.lower() == 'adam':\n",
        "        return optim.Adam(model.parameters(),\n",
        "                         lr=learning_rate,\n",
        "                         betas=(0.9, 0.999),\n",
        "                         weight_decay=1e-4)\n",
        "\n",
        "    elif optimizer_name.lower() == 'adamw':\n",
        "        return optim.AdamW(model.parameters(),\n",
        "                          lr=learning_rate,\n",
        "                          betas=(0.9, 0.999),\n",
        "                          weight_decay=1e-2)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
        "\n",
        "# Initialize optimizer - using AdamW as it generally provides better performance\n",
        "optimizer = get_optimizer(model, 'adamw', LEARNING_RATE)\n",
        "print(f\"Optimizer configuration: {optimizer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSkTDh_5CEZE"
      },
      "source": [
        "## Use Learning Rate Scheduling\n",
        "\n",
        "**Task**: Implement learning rate scheduling for improved training dynamics.\n",
        "\n",
        "**Requirements**:\n",
        "- Use StepLR or CosineAnnealingLR scheduler\n",
        "- Configure appropriate scheduling parameters\n",
        "- Track learning rate changes during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4z06UQcCEZE"
      },
      "outputs": [],
      "source": [
        "# Using CosineAnnealingLR for smooth learning rate decay\n",
        "# This provides better convergence compared to step-based schedules\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS, eta_min=1e-6)\n",
        "\n",
        "def get_current_lr(optimizer):\n",
        "    \"\"\"Get current learning rate from optimizer\"\"\"\n",
        "    return optimizer.param_groups[0]['lr']\n",
        "\n",
        "# Initialize list to track learning rates during training\n",
        "learning_rates = []\n",
        "\n",
        "print(f\"Learning rate scheduler: CosineAnnealingLR\")\n",
        "print(f\"Initial learning rate: {get_current_lr(optimizer)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L5oFqu6CEZE"
      },
      "source": [
        "## Apply Regularization Techniques  \n",
        "\n",
        "**Task**: Implement various regularization methods to prevent overfitting.\n",
        "\n",
        "**Requirements**:\n",
        "- Use dropout in your model (already included in architecture)\n",
        "- Implement early stopping mechanism\n",
        "- Add L2 weight decay (already in optimizer)\n",
        "- Optional: implement label smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHSUpKFsCEZE"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
        "\n",
        "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
        "\n",
        "        self.patience = patience # Number of epochs to wait before stopping if no improvement\n",
        "        self.min_delta = min_delta # Minimum change in the monitored quantity to qualify as an improvement\n",
        "        self.restore_best_weights = restore_best_weights # Whether to restore model weights from the best epoch\n",
        "        self.best_loss = None\n",
        "        self.counter = 0\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        \"\"\"\n",
        "        Check if training should stop based on validation loss\n",
        "        \"\"\"\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.save_checkpoint(model)\n",
        "        elif self.best_loss - val_loss > self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            self.save_checkpoint(model)\n",
        "        else:\n",
        "            self.counter += 1\n",
        "\n",
        "        if self.counter >= self.patience:\n",
        "            if self.restore_best_weights:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def save_checkpoint(self, model):\n",
        "        \"\"\"Save model weights\"\"\"\n",
        "        self.best_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "# Initialize early stopping with patience of 10 epochs\n",
        "early_stopping = EarlyStopping(patience=10, min_delta=0.001, restore_best_weights=True)\n",
        "\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    \"\"\"Label smoothing cross entropy loss for better generalization\"\"\"\n",
        "\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.confidence = 1.0 - smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pred: Predictions (batch_size, num_classes)\n",
        "            target: Ground truth labels (batch_size,)\n",
        "        \"\"\"\n",
        "        pred = pred.log_softmax(dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (pred.size(-1) - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=-1))\n",
        "\n",
        "# Use label smoothing for better generalization\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
        "print(f\"Loss function: Label Smoothing Cross Entropy (smoothing=0.1)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTunEDULCEZE"
      },
      "source": [
        "##  Training Loop with Advanced Features\n",
        "\n",
        "**Task**: Implement comprehensive training loop with all optimizations.\n",
        "\n",
        "**Requirements**:\n",
        "- Track multiple metrics during training\n",
        "- Implement proper validation\n",
        "- Save best model checkpoints\n",
        "- Monitor learning rate and loss curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC9YvrtiCEZE"
      },
      "outputs": [],
      "source": [
        "# Initialize tracking lists\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    \"\"\"Calculate accuracy from model outputs and true labels\"\"\"\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    total_batches = len(trainloader)\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Track metrics\n",
        "        running_loss += loss.item()\n",
        "        running_acc += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        # Print progress\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Batch [{batch_idx}/{total_batches}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / total_batches\n",
        "    epoch_acc = running_acc / total_batches\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, testloader, criterion, device):\n",
        "    \"\"\"Validate model for one epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    total_batches = len(testloader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_acc += calculate_accuracy(outputs, labels)\n",
        "\n",
        "    epoch_loss = running_loss / total_batches\n",
        "    epoch_acc = running_acc / total_batches\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting training...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'\\nEpoch [{epoch+1}/{NUM_EPOCHS}]')\n",
        "\n",
        "    # Training phase\n",
        "    train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
        "\n",
        "    # Validation phase\n",
        "    val_loss, val_acc = validate_epoch(model, testloader, criterion, device)\n",
        "\n",
        "    # Step scheduler\n",
        "    scheduler.step()\n",
        "    current_lr = get_current_lr(optimizer)\n",
        "\n",
        "    # Track metrics\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "    learning_rates.append(current_lr)\n",
        "\n",
        "    # Print epoch statistics\n",
        "    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "    print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "    print(f'Learning Rate: {current_lr:.6f}')\n",
        "\n",
        "    # Check early stopping\n",
        "    if early_stopping(val_loss, model):\n",
        "        print(f'Early stopping triggered at epoch {epoch+1}')\n",
        "        break\n",
        "\n",
        "training_time = time.time() - start_time\n",
        "print(f'\\nTraining completed in {training_time:.2f} seconds')\n",
        "\n",
        "# Plot training curves\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Loss curves\n",
        "ax1.plot(train_losses, label='Training Loss', color='blue')\n",
        "ax1.plot(val_losses, label='Validation Loss', color='red')\n",
        "ax1.set_title('Training and Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Accuracy curves\n",
        "ax2.plot(train_accuracies, label='Training Accuracy', color='blue')\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy', color='red')\n",
        "ax2.set_title('Training and Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy (%)')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Learning rate schedule\n",
        "ax3.plot(learning_rates, color='green')\n",
        "ax3.set_title('Learning Rate Schedule')\n",
        "ax3.set_xlabel('Epoch')\n",
        "ax3.set_ylabel('Learning Rate')\n",
        "ax3.grid(True)\n",
        "\n",
        "# Training progress\n",
        "epochs_run = len(train_losses)\n",
        "ax4.plot(range(epochs_run), [max(val_accuracies[:i+1]) for i in range(epochs_run)],\n",
        "         label='Best Validation Accuracy', color='purple')\n",
        "ax4.set_title('Best Validation Accuracy Progress')\n",
        "ax4.set_xlabel('Epoch')\n",
        "ax4.set_ylabel('Best Val Accuracy (%)')\n",
        "ax4.legend()\n",
        "ax4.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAd42dymCEZE"
      },
      "source": [
        "## Evaluate Model with Advanced Metrics\n",
        "\n",
        "**Task**: Comprehensive evaluation using multiple metrics and visualizations.\n",
        "\n",
        "**Requirements**:\n",
        "- Calculate accuracy, precision, recall, F1-score\n",
        "- Generate confusion matrix\n",
        "- Analyze per-class performance\n",
        "- Visualize misclassified examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCrhgr4bCEZE"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, testloader, device, class_names):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_outputs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_outputs.extend(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision_macro = precision_score(all_labels, all_predictions, average='macro')\n",
        "    recall_macro = recall_score(all_labels, all_predictions, average='macro')\n",
        "    f1_macro = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    precision_weighted = precision_score(all_labels, all_predictions, average='weighted')\n",
        "    recall_weighted = recall_score(all_labels, all_predictions, average='weighted')\n",
        "    f1_weighted = f1_score(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(\"==============================\")\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"==============================\")\n",
        "    print(f\"Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    print(f\"Macro Precision: {precision_macro:.4f}\")\n",
        "    print(f\"Macro Recall: {recall_macro:.4f}\")\n",
        "    print(f\"Macro F1-Score: {f1_macro:.4f}\")\n",
        "    print(f\"Weighted Precision: {precision_weighted:.4f}\")\n",
        "    print(f\"Weighted Recall: {recall_weighted:.4f}\")\n",
        "    print(f\"Weighted F1-Score: {f1_weighted:.4f}\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "    # Per-class accuracy\n",
        "    per_class_acc = cm.diagonal() / cm.sum(axis=1)\n",
        "    print(\"\\nPer-class Accuracy:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name}: {per_class_acc[i]:.4f} ({per_class_acc[i]*100:.2f}%)\")\n",
        "\n",
        "    return {\n",
        "        'predictions': all_predictions,\n",
        "        'labels': all_labels,\n",
        "        'outputs': all_outputs,\n",
        "        'confusion_matrix': cm,\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'per_class_accuracy': per_class_acc\n",
        "    }\n",
        "\n",
        "# Evaluate the model\n",
        "results = evaluate_model(model, testloader, device, CIFAR10_CLASSES)\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\nDETAILED CLASSIFICATION REPORT\")\n",
        "print(\"=================================================================\")\n",
        "print(classification_report(results['labels'], results['predictions'],\n",
        "                          target_names=CIFAR10_CLASSES, digits=4))\n",
        "\n",
        "# Create confusion matrix visualization\n",
        "plt.figure(figsize=(12, 10))\n",
        "cm_normalized = results['confusion_matrix'].astype('float') / results['confusion_matrix'].sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='Blues',\n",
        "            xticklabels=CIFAR10_CLASSES, yticklabels=CIFAR10_CLASSES)\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Raw confusion matrix with counts\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(results['confusion_matrix'], annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=CIFAR10_CLASSES, yticklabels=CIFAR10_CLASSES)\n",
        "plt.title('Confusion Matrix (Raw Counts)')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.xticks(rotation=45)\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "def find_misclassified_examples(model, testloader, device, class_names, num_examples=20):\n",
        "    \"\"\"Find and return misclassified examples for visualization\"\"\"\n",
        "    model.eval()\n",
        "    misclassified = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Find misclassified examples\n",
        "            wrong_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n",
        "\n",
        "            for idx in wrong_indices:\n",
        "                if len(misclassified) >= num_examples:\n",
        "                    break\n",
        "\n",
        "                misclassified.append({\n",
        "                    'image': inputs[idx].cpu(),\n",
        "                    'true_label': labels[idx].item(),\n",
        "                    'predicted_label': predicted[idx].item(),\n",
        "                    'confidence': probabilities[idx][predicted[idx]].item(),\n",
        "                    'true_confidence': probabilities[idx][labels[idx]].item()\n",
        "                })\n",
        "\n",
        "            if len(misclassified) >= num_examples:\n",
        "                break\n",
        "\n",
        "    return misclassified\n",
        "\n",
        "# Find misclassified examples\n",
        "misclassified_examples = find_misclassified_examples(model, testloader, device, CIFAR10_CLASSES, 16)\n",
        "\n",
        "# Visualize misclassified examples\n",
        "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, example in enumerate(misclassified_examples):\n",
        "    if i >= 16:\n",
        "        break\n",
        "\n",
        "    # Unnormalize image for display\n",
        "    img = example['image'].permute(1, 2, 0).numpy()\n",
        "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    axes[i].imshow(img)\n",
        "    true_class = CIFAR10_CLASSES[example['true_label']]\n",
        "    pred_class = CIFAR10_CLASSES[example['predicted_label']]\n",
        "    confidence = example['confidence']\n",
        "\n",
        "    axes[i].set_title(f'True: {true_class}\\nPred: {pred_class}\\nConf: {confidence:.3f}',\n",
        "                     color='red', fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Misclassified Examples', fontsize=16, y=0.98)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Analyze worst performing classes\n",
        "print(\"\\nWORST PERFORMING CLASSES\")\n",
        "print(\"=====================================\")\n",
        "worst_classes = np.argsort(results['per_class_accuracy'])[:3]\n",
        "for idx in worst_classes:\n",
        "    class_name = CIFAR10_CLASSES[idx]\n",
        "    accuracy = results['per_class_accuracy'][idx]\n",
        "    print(f\"{class_name}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvg1S4CZCEZF"
      },
      "source": [
        "##  Visualize Results\n",
        "\n",
        "**Task**: Create comprehensive visualizations of model performance and behavior.\n",
        "\n",
        "**Requirements**:\n",
        "- Plot training/validation curves\n",
        "- Visualize model predictions\n",
        "- Show sample activations or feature maps\n",
        "- Create performance comparison charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXf_lSXICEZF"
      },
      "outputs": [],
      "source": [
        "def plot_comprehensive_results(train_losses, val_losses, train_accs, val_accs,\n",
        "                             learning_rates, results, class_names):\n",
        "    \"\"\"Create comprehensive visualization of all results\"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 15))\n",
        "\n",
        "    # Training curves\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    plt.plot(train_losses, label='Training Loss', color='blue', linewidth=2)\n",
        "    plt.plot(val_losses, label='Validation Loss', color='red', linewidth=2)\n",
        "    plt.title('Loss Curves', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    plt.plot(train_accs, label='Training Accuracy', color='blue', linewidth=2)\n",
        "    plt.plot(val_accs, label='Validation Accuracy', color='red', linewidth=2)\n",
        "    plt.title('Accuracy Curves', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    plt.plot(learning_rates, color='green', linewidth=2)\n",
        "    plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Per-class performance\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    bars = plt.bar(range(len(class_names)), results['per_class_accuracy'],\n",
        "                   color='skyblue', edgecolor='navy', alpha=0.7)\n",
        "    plt.title('Per-Class Accuracy', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, acc in zip(bars, results['per_class_accuracy']):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "    # Training progress metrics\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    best_val_acc = [max(val_accs[:i]) for i in range(1, len(val_accs) + 1)]\n",
        "    plt.plot(epochs, best_val_acc, label='Best Val Accuracy', color='purple', linewidth=2)\n",
        "    plt.plot(epochs, val_accs, label='Current Val Accuracy', color='orange', alpha=0.7)\n",
        "    plt.title('Validation Accuracy Progress', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss difference (overfitting indicator)\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    loss_diff = np.array(train_losses) - np.array(val_losses)\n",
        "    plt.plot(loss_diff, color='red', linewidth=2)\n",
        "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "    plt.title('Training - Validation Loss\\n(Overfitting Indicator)', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss Difference')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Final metrics summary\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "    values = [results['accuracy'], results['precision_macro'],\n",
        "              results['recall_macro'], results['f1_macro']]\n",
        "\n",
        "    bars = plt.bar(metrics, values, color=['gold', 'lightcoral', 'lightblue', 'lightgreen'],\n",
        "                   edgecolor='black', alpha=0.8)\n",
        "    plt.title('Final Model Metrics', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Score')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    for bar, val in zip(bars, values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{val:.3f}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
        "\n",
        "    # Confusion matrix (smaller version)\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    cm_norm = results['confusion_matrix'].astype('float') / results['confusion_matrix'].sum(axis=1)[:, np.newaxis]\n",
        "    im = plt.imshow(cm_norm, interpolation='nearest', cmap='Blues')\n",
        "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "    plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "\n",
        "    # Training time and efficiency\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    final_train_acc = train_accs[-1]\n",
        "    final_val_acc = val_accs[-1]\n",
        "    best_val_acc_final = max(val_accs)\n",
        "\n",
        "    categories = ['Final\\nTrain Acc', 'Final\\nVal Acc', 'Best\\nVal Acc']\n",
        "    acc_values = [final_train_acc, final_val_acc, best_val_acc_final]\n",
        "\n",
        "    bars = plt.bar(categories, acc_values, color=['steelblue', 'darkorange', 'forestgreen'],\n",
        "                   alpha=0.8, edgecolor='black')\n",
        "    plt.title('Training Summary', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    for bar, val in zip(bars, acc_values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
        "                f'{val:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create comprehensive visualization\n",
        "plot_comprehensive_results(train_losses, val_losses, train_accuracies, val_accuracies,\n",
        "                          learning_rates, results, CIFAR10_CLASSES)\n",
        "\n",
        "def visualize_predictions_with_confidence(model, testloader, device, class_names, num_samples=12):\n",
        "    \"\"\"Visualize model predictions with confidence scores\"\"\"\n",
        "    model.eval()\n",
        "    images_shown = 0\n",
        "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in testloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            for i in range(inputs.size(0)):\n",
        "                if images_shown >= num_samples:\n",
        "                    break\n",
        "\n",
        "                # Unnormalize image\n",
        "                img = inputs[i].permute(1, 2, 0).cpu().numpy()\n",
        "                img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "                img = np.clip(img, 0, 1)\n",
        "\n",
        "                axes[images_shown].imshow(img)\n",
        "\n",
        "                true_label = labels[i].item()\n",
        "                pred_label = predicted[i].item()\n",
        "                confidence = probabilities[i][pred_label].item()\n",
        "\n",
        "                # Color code: green for correct, red for incorrect\n",
        "                color = 'green' if true_label == pred_label else 'red'\n",
        "\n",
        "                title = f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\\nConf: {confidence:.3f}'\n",
        "                axes[images_shown].set_title(title, color=color, fontsize=10)\n",
        "                axes[images_shown].axis('off')\n",
        "\n",
        "                images_shown += 1\n",
        "\n",
        "            if images_shown >= num_samples:\n",
        "                break\n",
        "\n",
        "    plt.suptitle('Model Predictions with Confidence Scores\\n(Green: Correct, Red: Incorrect)',\n",
        "                 fontsize=16, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize predictions with confidence\n",
        "visualize_predictions_with_confidence(model, testloader, device, CIFAR10_CLASSES)\n",
        "\n",
        "def visualize_feature_maps(model, testloader, device, layer_name='inception1'):\n",
        "    \"\"\"Visualize feature maps from a specific layer\"\"\"\n",
        "    # Hook to capture feature maps\n",
        "    feature_maps = []\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        feature_maps.append(output.cpu())\n",
        "\n",
        "    # Register hook on the specified layer\n",
        "    if layer_name == 'inception1':\n",
        "        hook = model.inception1.register_forward_hook(hook_fn)\n",
        "    elif layer_name == 'inception2':\n",
        "        hook = model.inception2.register_forward_hook(hook_fn)\n",
        "    else:\n",
        "        print(f\"Layer {layer_name} not found\")\n",
        "        return\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Get one batch\n",
        "        inputs, labels = next(iter(testloader))\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Forward pass to trigger hook\n",
        "        _ = model(inputs)\n",
        "\n",
        "    # Remove hook\n",
        "    hook.remove()\n",
        "\n",
        "    # Visualize feature maps for first image\n",
        "    if feature_maps:\n",
        "        features = feature_maps[0][0]  # First image, all channels\n",
        "\n",
        "        # Select first 16 channels to visualize\n",
        "        num_features = min(16, features.shape[0])\n",
        "        fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i in range(num_features):\n",
        "            feature_map = features[i].numpy()\n",
        "            axes[i].imshow(feature_map, cmap='viridis')\n",
        "            axes[i].set_title(f'Channel {i}', fontsize=10)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        # Hide unused subplots\n",
        "        for i in range(num_features, 16):\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.suptitle(f'Feature Maps from {layer_name}', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Visualize feature maps from inception1 layer\n",
        "visualize_feature_maps(model, testloader, device, 'inception1')\n",
        "\n",
        "# Model architecture summary\n",
        "def print_model_summary(model, input_size=(3, 32, 32)):\n",
        "    \"\"\"Print detailed model summary\"\"\"\n",
        "    print(\"\\n\" + \"===============================\")\n",
        "    print(\"MODEL ARCHITECTURE SUMMARY\")\n",
        "    print(\"==================================\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Total Parameters: {total_params:,}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "    print(f\"Model Size (MB): {total_params * 4 / (1024 * 1024):.2f}\")\n",
        "\n",
        "    # Calculate model complexity\n",
        "    dummy_input = torch.randn(1, *input_size).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(dummy_input)\n",
        "\n",
        "    print(f\"Input Shape: {input_size}\")\n",
        "    print(f\"Output Shape: {output.shape[1:]}\")\n",
        "    print(f\"Number of Classes: {output.shape[1]}\")\n",
        "\n",
        "    print(\"\\nLayer-wise Parameter Count:\")\n",
        "    print(\"--------------------------------------\")\n",
        "    for name, module in model.named_modules():\n",
        "        if len(list(module.children())) == 0:  # Leaf modules only\n",
        "            params = sum(p.numel() for p in module.parameters())\n",
        "            if params > 0:\n",
        "                print(f\"{name:30} {params:>10,}\")\n",
        "\n",
        "print_model_summary(model)\n",
        "\n",
        "# Final results summary\n",
        "print(\"\\nFINAL TRAINING SUMMARY\")\n",
        "print(\"========================================================\")\n",
        "print(f\"Training completed in {len(train_losses)} epochs\")\n",
        "print(f\"Best validation accuracy: {max(val_accuracies):.2f}%\")\n",
        "print(f\"Final validation accuracy: {val_accuracies[-1]:.2f}%\")\n",
        "print(f\"Test accuracy: {results['accuracy']*100:.2f}%\")\n",
        "print(f\"Total training time: {training_time:.2f} seconds\")\n",
        "print(f\"Average time per epoch: {training_time/len(train_losses):.2f} seconds\")\n",
        "\n",
        "# Check if target accuracy achieved\n",
        "target_accuracy = 80.0\n",
        "if results['accuracy'] * 100 >= target_accuracy:\n",
        "  print(\"\\n\")\n",
        "else:\n",
        "    print(f\"\\nTarget accuracy of {target_accuracy}% not achieved.\")\n",
        "    print(f\"Your model achieved {results['accuracy']*100:.2f}% accuracy\")\n",
        "\n",
        "print(f\"Final model performance: {results['accuracy']*100:.2f}% accuracy\")\n",
        "\n",
        "# Save final model\n",
        "torch.save(model.state_dict(), 'inception_cnn_cifar10.pth')\n",
        "print(f\"\\nModel saved as 'inception_cnn_cifar10.pth'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
